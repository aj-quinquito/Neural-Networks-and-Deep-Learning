{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_with_vgg16_backbone(input_shape=(128, 128, 3), num_classes=13):\n",
    "    # Load VGG16 as the encoder\n",
    "    vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    vgg16_base.trainable = False  # Freeze the encoder layers\n",
    "\n",
    "    # Encoder - Use VGG16 layers for downsampling\n",
    "    c1 = vgg16_base.get_layer(\"block1_conv2\").output\n",
    "    c2 = vgg16_base.get_layer(\"block2_conv2\").output\n",
    "    c3 = vgg16_base.get_layer(\"block3_conv3\").output\n",
    "    c4 = vgg16_base.get_layer(\"block4_conv3\").output\n",
    "    c5 = vgg16_base.get_layer(\"block5_conv3\").output\n",
    "\n",
    "    # Decoder - Upsampling Path\n",
    "    u1 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u1 = layers.concatenate([u1, c4])\n",
    "    u1 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u1)\n",
    "    u1 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u1)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(u1)\n",
    "    u2 = layers.concatenate([u2, c3])\n",
    "    u2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
    "    u2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
    "\n",
    "    u3 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u2)\n",
    "    u3 = layers.concatenate([u3, c2])\n",
    "    u3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\n",
    "    u3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\n",
    "\n",
    "    u4 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(u3)\n",
    "    u4 = layers.concatenate([u4, c1])\n",
    "    u4 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u4)\n",
    "    u4 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u4)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(u4)\n",
    "\n",
    "    # Define the model\n",
    "    model = models.Model(inputs=vgg16_base.input, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and summarize the model\n",
    "vgg16_unet_model = unet_with_vgg16_backbone()\n",
    "vgg16_unet_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_with_resnet50_backbone(input_shape=(128, 128, 3), num_classes=3):\n",
    "    # Load ResNet50 as the encoder\n",
    "    resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    resnet50_base.trainable = False  # Freeze the encoder layers\n",
    "\n",
    "    # Encoder - Use ResNet50 layers for downsampling\n",
    "    c1 = resnet50_base.get_layer(\"conv1_relu\").output\n",
    "    c2 = resnet50_base.get_layer(\"conv2_block3_out\").output\n",
    "    c3 = resnet50_base.get_layer(\"conv3_block4_out\").output\n",
    "    c4 = resnet50_base.get_layer(\"conv4_block6_out\").output\n",
    "    c5 = resnet50_base.get_layer(\"conv5_block3_out\").output\n",
    "\n",
    "    # Decoder - Upsampling Path\n",
    "    u1 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u1 = layers.concatenate([u1, c4])\n",
    "    u1 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u1)\n",
    "    u1 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u1)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(u1)\n",
    "    u2 = layers.concatenate([u2, c3])\n",
    "    u2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
    "    u2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
    "\n",
    "    u3 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u2)\n",
    "    u3 = layers.concatenate([u3, c2])\n",
    "    u3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\n",
    "    u3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\n",
    "\n",
    "    u4 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(u3)\n",
    "    u4 = layers.concatenate([u4, c1])\n",
    "    u4 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u4)\n",
    "    u4 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u4)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(u4)\n",
    "\n",
    "    # Define the model\n",
    "    model = models.Model(inputs=resnet50_base.input, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and summarize the model\n",
    "resnet50_unet_model = unet_with_resnet50_backbone()\n",
    "resnet50_unet_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to images and masks\n",
    "train_image_path = 'path_to_train_images'\n",
    "train_mask_path = 'path_to_train_masks'\n",
    "val_image_path = 'path_to_val_images'\n",
    "val_mask_path = 'path_to_val_masks'\n",
    "\n",
    "# Create data generators\n",
    "train_image_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    train_image_path, target_size=(128, 128), class_mode=None, batch_size=32, seed=1)\n",
    "\n",
    "train_mask_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    train_mask_path, target_size=(128, 128), class_mode=None, batch_size=32, seed=1)\n",
    "\n",
    "val_image_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    val_image_path, target_size=(128, 128), class_mode=None, batch_size=32, seed=1)\n",
    "\n",
    "val_mask_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    val_mask_path, target_size=(128, 128), class_mode=None, batch_size=32, seed=1)\n",
    "\n",
    "# Combine generators for input images and masks\n",
    "train_gen = zip(train_image_gen, train_mask_gen)\n",
    "val_gen = zip(val_image_gen, val_mask_gen)\n",
    "\n",
    "# Train the model\n",
    "history = vgg16_unet_model.fit(train_gen, validation_data=val_gen, epochs=50, steps_per_epoch=100, validation_steps=25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
