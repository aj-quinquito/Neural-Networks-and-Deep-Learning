{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da1ec96-253d-4176-84d0-bc0136d21f24",
   "metadata": {},
   "source": [
    "# **Assignment 4**\n",
    "## **Group 3:**\n",
    "* Karyl Grasparil\n",
    "* Mariah Quinquito\n",
    "* Yanna Polonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9aeeb3b-58e5-44c2-bc6f-41177f555959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import tokenize\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718e4aa-d5fc-4a92-b898-82cc37f0f244",
   "metadata": {},
   "source": [
    "# **1. Data Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750f2dd-fd14-4f81-a83a-b48dc34c9936",
   "metadata": {},
   "source": [
    "## Using nltk.corpus.gutenberg.raw to load the entire text of plays: 1) The Tragedy of Hamlet, Prince of Denmark, 2) The Tragedy of Macbeth, and 3) The Tragedy of Julius Caesar into a single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bee3d3f-2aa3-4b52-8492-e24bb88a184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/ypolonia/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading Gutenberg corpus\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Exploring file identifiers included in the Gutenberg corpus\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f958ca0-80a5-4df9-ba14-dc7e1ffa222a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rall honest thought,\n",
      "And common good to all, made one of them.\n",
      "His life was gentle, and the Elements\n",
      "So mixt in him, that Nature might stand vp,\n",
      "And say to all the world; This was a man\n",
      "\n",
      "   Octa. According to his Vertue, let vs vse him\n",
      "Withall Respect, and Rites of Buriall.\n",
      "Within my Tent his bones to night shall ly,\n",
      "Most like a Souldier ordered Honourably:\n",
      "So call the Field to rest, and let's away,\n",
      "To part the glories of this happy day.\n",
      "\n",
      "Exeunt. omnes.\n",
      "\n",
      "\n",
      "FINIS. THE TRAGEDIE OF IVLIVS CaeSAR.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding each file name into the variable plays_raw\n",
    "plays_raw = gutenberg.raw('shakespeare-hamlet.txt') + gutenberg.raw('shakespeare-macbeth.txt') + gutenberg.raw('shakespeare-caesar.txt')\n",
    "\n",
    "# Printing the last 500 characters in plays_raw\n",
    "print(plays_raw[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d368942-0073-400c-bfcd-027489938133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[the tragedie of hamlet by william shakespeare 1599]\\n\\n\\nactus primus. scoena prima.\\n\\nenter barnardo a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lowering the case of play_raw content\n",
    "plays_raw = plays_raw.lower()\n",
    "\n",
    "# Printing the first 100 characters to validate case changes made\n",
    "plays_raw[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d63c27-8878-4de5-80db-9cd9de6d53bc",
   "metadata": {},
   "source": [
    "## Using Speller from the autocorrect library to correct spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac49619-9357-4d38-8b7c-b772ee5da3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "speller = Speller(lang = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99076a29-a9ae-4df3-8e5a-3158aa26a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_corrected = speller(plays_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ffe9ea-a829-4f57-a58d-f8e9f1593b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a general honest thought,\\nand common good to all, made one of them.\\nhis life was gentle, and the elements\\nso mix in him, that nature might stand vp,\\nand say to all the world; this was a man\\n\\n   oct. according to his virtue, let vs vse him\\nwithall respect, and rites of burial.\\nwithin my tent his bones to night shall ly,\\nmost like a soldier ordered honourable:\\nso call the field to rest, and let's away,\\nto part the stories of this happy day.\\n\\nevent. ones.\\n\\n\\nfinish. the tragedies of ivlivs cesar.\\n\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plays_corrected[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9780073-c0a2-40b8-9ebb-e2ad06a2e411",
   "metadata": {},
   "source": [
    "## Tokenizing the text into sentences, and then each sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508f39a2-f291-43f0-a77f-0950629e01f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ypolonia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the Punkt tokenizer module to ease text splitting into sentences and words, and be able to handle tab-separated values.\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc645ed-7afc-49ec-945a-c7e4155e1715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 5325)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing a sentence-tokenized copy of plays_raw in sentences_text\n",
    "sentences_txt = tokenize.sent_tokenize(plays_raw)\n",
    "\n",
    "# Verifying sentences_txt data type and its length \n",
    "type(sentences_txt), len(sentences_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1660e086-8726-4a2e-9e20-03f105fb84c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[the tragedie of hamlet by william shakespeare 1599]\\n\\n\\nactus primus.',\n",
       " 'scoena prima.',\n",
       " 'enter barnardo and francisco two centinels.',\n",
       " 'barnardo.',\n",
       " \"who's there?\",\n",
       " 'fran.',\n",
       " 'nay answer me: stand & vnfold\\nyour selfe\\n\\n   bar.',\n",
       " 'long liue the king\\n\\n   fran.',\n",
       " 'barnardo?',\n",
       " 'bar.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 10 sentences\n",
    "sentences_txt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c64a897-5d1f-4f43-abc8-93766ae9da29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing the tokenized words in each sentence as list named word_text\n",
    "words_txt = [tokenize.word_tokenize(sent) for sent in sentences_txt]\n",
    "\n",
    "# Verifying words_txt list elements data type and the length of its first element\n",
    "type(words_txt[0]), len(words_txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2eff562-8b57-49aa-8c28-1c90575eec09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'the',\n",
       " 'tragedie',\n",
       " 'of',\n",
       " 'hamlet',\n",
       " 'by',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " '1599',\n",
       " ']',\n",
       " 'actus',\n",
       " 'primus',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the words in the first sentence\n",
    "words_txt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91873fa0-b193-4614-a9cd-3c0606844a23",
   "metadata": {},
   "source": [
    "## Using regular expressions (the re library) to do some cleanup of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c02a5491-bfd2-4ba3-b26e-3684de3931b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(word):\n",
    "    \"\"\"\n",
    "    Function used to clean one word at a time\n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing words with one to two characters\n",
    "    word = re.sub('\\\\b\\w{1,2}\\\\b', '', word)\n",
    "\n",
    "    # Remove punctuation using regex, keeping only word characters and whitespace\n",
    "    word = re.sub(r'[^\\w]', '', word)\n",
    "    \n",
    "    # Removing numbers, so we are just removing numbers that are three digits or more thanks to previous part\n",
    "    word = re.sub(\"\\d\", \"\", word)\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e31416-02df-4e9c-913d-b6aad032a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = []\n",
    "cleaned_words = []\n",
    "\n",
    "for sentence in sentences_txt:\n",
    "    # Splitting each setence into string elements\n",
    "    words_sentences = sentence.split()\n",
    "\n",
    "    # Applying the clean_text function to every splitted setence\n",
    "    cleaned_sentence = [clean_text(word) for word in words_sentences]\n",
    "    \n",
    "    # Removing empty strings in each setence if any\n",
    "    cleaned_sentence = [item for item in cleaned_sentence if item]\n",
    "\n",
    "    # Appending each sentence to cleaned_sentences\n",
    "    cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    # Extracting from each setence (sublist) the words to append flatten list named cleaned_words\n",
    "    cleaned_words = [item for sublist in cleaned_sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d44f0ab-48f7-4f43-9839-b99a32bcf226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'tragedie', 'hamlet', 'william', 'shakespeare']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 5 cleaned words\n",
    "cleaned_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d38290-a068-4f14-80cf-a3063f317844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'tragedie', 'hamlet', 'william', 'shakespeare', 'actus', 'primus'],\n",
       " ['scoena', 'prima']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 2 cleaned setences\n",
    "cleaned_sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15293f5b-a04a-4d18-ae22-52cdbfaeabdb",
   "metadata": {},
   "source": [
    "## Creating a list of stopwords (using publicly available lists and/or adding your own) and removing these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7303831b-acbd-44b0-9c7f-2e4cce3155c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stop words from https://github.com/stopwords-iso/stopwords-en/blob/master/stopwords-en.txt, which are three or more characters\n",
    "modern_eng_stop = [\"'ll\", \"'tis\", \"'twas\", \"'ve\", \"a's\", \"able\", \"ableabout\", \"about\", \"above\", \"abroad\", \"abst\", \"accordance\", \"according\",\\\n",
    "             \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"adopted\", \"affected\", \"affecting\", \"affects\", \\\n",
    "             \"after\", \"afterwards\", \"again\", \"against\", \"ago\", \"ahead\", \"ain't\", \"aint\", \"all\", \"allow\", \"allows\", \\\n",
    "             \"almost\", \"alone\", \"along\", \"alongside\", \"already\", \"also\", \"although\", \"always\", \"amid\", \"amidst\", \"among\", \"amongst\", \\\n",
    "             \"amoungst\", \"amount\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \\\n",
    "             \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \\\n",
    "             \"are\", \"area\", \"areas\", \"aren\", \"aren't\", \"arent\", \"arise\", \"around\", \"arpa\", \"aside\", \"ask\", \"asked\", \"asking\", \\\n",
    "             \"asks\", \"associated\", \"at\", \"au\", \"auth\", \"available\", \"aw\", \"away\", \"awfully\", \"az\", \"b\", \"ba\", \"back\", \"backed\", \"backing\", \\\n",
    "             \"backs\", \"backward\", \"backwards\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \\\n",
    "             \"beforehand\", \"began\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"beings\", \"believe\", \"below\", \\\n",
    "             \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"big\", \"bill\", \"billion\", \"biol\", \"both\", \"bottom\", \"brief\", \\\n",
    "             \"briefly\", \"but\", \"buy\", \"c'mon\", \"c's\", \"call\", \"came\", \"can\", \"can't\", \"cannot\", \"cant\", \"caption\", \"case\", \"cases\", \\\n",
    "             \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\", \"clear\", \"clearly\", \"click\", \"cmon\", \"co.\", \"com\", \"come\", \"comes\", \\\n",
    "             \"computer\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"copy\", \\\n",
    "             \"corresponding\", \"could\", \"could've\", \"couldn\", \"couldn't\", \"couldnt\", \"course\", \"cry\", \"currently\", \"dare\", \"daren't\", \\\n",
    "             \"darent\", \"date\", \"dear\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"did\", \"didn\", \"didn't\", \"didnt\", \\\n",
    "             \"differ\", \"different\", \"differently\", \"directly\", \"does\", \"doesn\", \"doesn't\", \"doesnt\", \"doing\", \"don\", \"don't\", \"done\", \\\n",
    "             \"dont\", \"doubtful\", \"down\", \"downed\", \"downing\", \"downs\", \"downwards\", \"due\", \"during\", \"each\", \"early\", \"edu\", \"effect\", \\\n",
    "             \"eight\", \"eighty\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"end\", \"ended\", \"ending\", \"ends\", \"enough\", \"entirely\", \\\n",
    "             \"especially\", \"et-al\", \"etc\", \"even\", \"evenly\", \"ever\", \"evermore\", \"every\", \"everybody\", \"everyone\", \"everything\", \\\n",
    "             \"everywhere\", \"exactly\", \"example\", \"except\", \"face\", \"faces\", \"fact\", \"facts\", \"fairly\", \"far\", \"farther\", \"felt\", \"few\", \\\n",
    "             \"fewer\", \"fifteen\", \"fifth\", \"fifty\", \"fify\", \"fill\", \"find\", \"finds\", \"fire\", \"first\", \"five\", \"fix\", \"followed\", \\\n",
    "             \"following\", \"follows\", \"for\", \"forever\", \"former\", \"formerly\", \"forth\", \"forty\", \"forward\", \"found\", \"four\", \"free\", \\\n",
    "             \"from\", \"front\", \"full\", \"fully\", \"further\", \"furthered\", \"furthering\", \"furthermore\", \"furthers\", \"gave\", \"general\", \\\n",
    "             \"generally\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"gmt\", \"goes\", \"going\", \"gone\", \"good\", \"goods\", \\\n",
    "             \"got\", \"gotten\", \"gov\", \"great\", \"greater\", \"greatest\", \"greetings\", \"group\", \"grouped\", \"grouping\", \"groups\", \"had\", \\\n",
    "             \"hadn't\", \"hadnt\", \"half\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasn't\", \"hasnt\", \"have\", \"haven\", \"haven't\", \"havent\", \\\n",
    "             \"having\", \"he'd\", \"he'll\", \"he's\", \"hed\", \"hell\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"here's\", \"hereafter\", \"hereby\", \\\n",
    "             \"herein\", \"heres\", \"hereupon\", \"hers\", \"herself\", \"herse”\", \"hes\", \"hid\", \"high\", \"higher\", \"highest\", \"him\", \"himself\", \\\n",
    "             \"himse”\", \"his\", \"hither\", \"home\", \"homepage\", \"hopefully\", \"how\", \"how'd\", \"how'll\", \"how's\", \"howbeit\", \"however\", \"htm\", \\\n",
    "             \"html\", \"http\", \"hundred\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"i.e.\", \"ignored\", \"ill\", \"immediate\", \"immediately\", \"importance\", \\\n",
    "             \"important\", \"inasmuch\", \"inc\", \"inc.\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \\\n",
    "             \"inside\", \"insofar\", \"instead\", \"int\", \"interest\", \"interested\", \"interesting\", \"interests\", \"into\", \"invention\", \"inward\", \\\n",
    "             \"isn\", \"isn't\", \"isnt\", \"it'd\", \"it'll\", \"it's\", \"itd\", \"itll\", \"its\", \"itself\", \"itse”\", \"ive\", \"join\", \"just\", \"keep\", \\\n",
    "             \"keeps\", \"kept\", \"keys\", \"kind\", \"knew\", \"know\", \"known\", \"knows\", \"large\", \"largely\", \"last\", \"lately\", \"later\", \"latest\", \\\n",
    "             \"latter\", \"latterly\", \"least\", \"length\", \"less\", \"lest\", \"let\", \"let's\", \"lets\", \"like\", \"liked\", \"likely\", \"likewise\", \\\n",
    "             \"line\", \"little\", \"long\", \"longer\", \"longest\", \"look\", \"looking\", \"looks\", \"low\", \"lower\", \"ltd\", \"made\", \"mainly\", \"make\", \\\n",
    "             \"makes\", \"making\", \"man\", \"many\", \"may\", \"maybe\", \"mayn't\", \"maynt\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"member\", \\\n",
    "             \"members\", \"men\", \"merely\", \"microsoft\", \"might\", \"might've\", \"mightn't\", \"mightnt\", \"mil\", \"mill\", \"million\", \"mine\", \\\n",
    "             \"minus\", \"miss\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mrs\", \"msie\", \"much\", \"mug\", \"must\", \"must've\", \"mustn't\", \\\n",
    "             \"mustnt\", \"myself\", \"myse”\", \"name\", \"namely\", \"nay\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needed\", \\\n",
    "             \"needing\", \"needn't\", \"neednt\", \"needs\", \"neither\", \"net\", \"netscape\", \"never\", \"neverf\", \"neverless\", \"nevertheless\", \\\n",
    "             \"new\", \"newer\", \"newest\", \"next\", \"nine\", \"ninety\", \"no-one\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \\\n",
    "             \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"notwithstanding\", \"novel\", \"now\", \"nowhere\", \"null\", \"number\", \"numbers\", \\\n",
    "             \"obtain\", \"obtained\", \"obviously\", \"off\", \"often\", \"okay\", \"old\", \"older\", \"oldest\", \"omitted\", \"once\", \"one\", \"one's\", \\\n",
    "             \"ones\", \"only\", \"onto\", \"open\", \"opened\", \"opening\", \"opens\", \"opposite\", \"ord\", \"order\", \"ordered\", \"ordering\", \"orders\", \\\n",
    "             \"org\", \"other\", \"others\", \"otherwise\", \"ought\", \"oughtn't\", \"oughtnt\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \\\n",
    "             \"overall\", \"owing\", \"own\", \"page\", \"pages\", \"part\", \"parted\", \"particular\", \"particularly\", \"parting\", \"parts\", \"past\",\\\n",
    "             \"per\", \"perhaps\", \"place\", \"placed\", \"places\", \"please\", \"plus\", \"pmid\", \"point\", \"pointed\", \"pointing\", \"points\", \"poorly\", \\\n",
    "             \"possible\", \"possibly\", \"potentially\", \"predominantly\", \"present\", \"presented\", \"presenting\", \"presents\", \"presumably\", \\\n",
    "             \"previously\", \"primarily\", \"probably\", \"problem\", \"problems\", \"promptly\", \"proud\", \"provided\", \"provides\", \"put\", \"puts\", \\\n",
    "             \"que\", \"quickly\", \"quite\", \"ran\", \"rather\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \\\n",
    "             \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"reserved\", \"respectively\", \"resulted\", \\\n",
    "             \"resulting\", \"results\", \"right\", \"ring\", \"room\", \"rooms\", \"round\", \"run\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \\\n",
    "             \"sec\", \"second\", \"secondly\", \"seconds\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sees\", \\\n",
    "             \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"seventy\", \"several\", \"shall\", \"shan't\", \"shant\", \\\n",
    "             \"she\", \"she'd\", \"she'll\", \"she's\", \"shed\", \"shell\", \"shes\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"shouldnt\", \"show\", \\\n",
    "             \"showed\", \"showing\", \"shown\", \"showns\", \"shows\", \"side\", \"sides\", \"significant\", \"significantly\", \"similar\", \"similarly\", \\\n",
    "             \"since\", \"sincere\", \"site\", \"six\", \"sixty\", \"slightly\", \"small\", \"smaller\", \"smallest\", \"some\", \"somebody\", \"someday\", \\\n",
    "             \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \\\n",
    "             \"specifically\", \"specified\", \"specify\", \"specifying\", \"state\", \"states\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \\\n",
    "             \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"system\", \"t's\", \"take\", \"taken\", \"taking\", \"tell\", \"ten\", \\\n",
    "             \"tends\", \"test\", \"text\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"that's\", \"that've\", \"thatll\", \"thats\", \\\n",
    "             \"thatve\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"there'd\", \"there'll\", \"there're\", \\\n",
    "             \"there's\", \"there've\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"therell\", \"thereof\", \"therere\", \"theres\", \\\n",
    "             \"thereto\", \"thereupon\", \"thereve\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"theyd\", \"theyll\", \"theyre\", \\\n",
    "             \"theyve\", \"thick\", \"thin\", \"thing\", \"things\", \"think\", \"thinks\", \"third\", \"thirty\", \"this\", \"thorough\", \"thoroughly\", \\\n",
    "             \"those\", \"thou\", \"though\", \"thoughh\", \"thought\", \"thoughts\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \\\n",
    "             \"thus\", \"til\", \"till\", \"tip\", \"tis\", \"today\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tried\", \"tries\", \\\n",
    "             \"trillion\", \"truly\", \"try\", \"trying\", \"turn\", \"turned\", \"turning\", \"turns\", \"twas\", \"twelve\", \"twenty\", \"twice\", \"two\", \\\n",
    "             \"under\", \"underneath\", \"undoing\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"upon\", \"ups\", \\\n",
    "             \"upwards\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"uucp\", \"value\", \"various\", \\\n",
    "             \"versus\", \"very\", \"via\", \"viz\", \"vol\", \"vols\", \"want\", \"wanted\", \"wanting\", \"wants\", \"was\", \"wasn\", \"wasn't\", \"wasnt\", \"way\", \\\n",
    "             \"ways\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"web\", \"webpage\", \"website\", \"wed\", \"welcome\", \"well\", \"wells\", \"went\", \\\n",
    "             \"were\", \"weren\", \"weren't\", \"werent\", \"weve\", \"wf\", \"what\", \"what'd\", \"what'll\", \"what's\", \"what've\", \"whatever\", \\\n",
    "             \"whatll\", \"whats\", \"whatve\", \"when\", \"when'd\", \"when'll\", \"when's\", \"whence\", \"whenever\", \"where\", \"where'd\", \"where'll\", \\\n",
    "             \"where's\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \\\n",
    "             \"whichever\", \"while\", \"whilst\", \"whim\", \"whither\", \"who\", \"who'd\", \"who'll\", \"who's\", \"whod\", \"whoever\", \"whole\", \\\n",
    "             \"wholl\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"why'd\", \"why'll\", \"why's\", \"widely\", \"width\", \"will\", \"willing\", \\\n",
    "             \"wish\", \"with\", \"within\", \"without\", \"won\", \"won't\", \"wonder\", \"wont\", \"words\", \"work\", \"worked\", \"working\", \"works\", \\\n",
    "             \"world\", \"would\", \"would've\", \"wouldn\", \"wouldn't\", \"wouldnt\", \"www\", \"year\", \"years\", \"yes\", \"yet\", \"you\", \"you'd\", \\\n",
    "             \"you'll\", \"you're\", \"you've\", \"youd\", \"youll\", \"young\", \"younger\", \"youngest\", \"your\", \"youre\", \"yours\", \"yourself\", \\\n",
    "             \"yourselves\", \"youve\", \"zero\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c77ce8e1-d528-462e-aa32-84457f84d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old English stop words provided by ChatGPT (those which are 3+ characters are the considered ones): þe - the\", \"þæt - that\", \"þes - this\",\n",
    "# \"þa - those\", \"ne - not\", \"ac - but\", \"gif - if\", \"sē - the (that)\", \"þū - you\", \"ic - I\", \"eall - all\", \"nān - none\", \"þīn - your\", \n",
    "# \"wē - we\", \"ge - you (plural)\", \"hē - he\", \"hēo - she\", \"swā - so\", \"mid - with\", \"fram - from\", \"þurh - through\", \"tō - to\", \"þæt - that\",\n",
    "# \"hwæðer - whether\", \"līc - body\", \"witan - to know\", \"secgan - to say\", \"cunnan - to be able\", \"don - to do\", \"bēon - to be\", \n",
    "# \"witan - to know\", \"hæbban - to have\", \"þearf - need\", \"hī - they\", \"þeah - although\", \"þa - the (those)\", \"sīe - may be\", \"wæs - was\", \n",
    "# \"eom - am\", \"hæfde - had\", \"ġē - you (plural)\", \"þa - then\", \"þæt - that (it)\", \"būtan - without\", \"eallunga - altogether\", \n",
    "# \"þū - thou (you)\", \"nǣfre - never\", \"hwæt - what\", \"hwī - why\", \"þæt - that\", \"læt - let\", \"wite - to know\", \"wæron - were\", \"swa - such\",\n",
    "# \"æfre - ever\", \"hū - how\", \"wyrcan - to work\", \"þinc - to think\", \"gesīen - to see\", \"cūðe - known\", \"hæle - to help\", \"beon - to be\", \n",
    "# \"sēo - she (the)\", \"cū - cow (to know)\", \"dǣl - part\", \"feorr - far\", \"mē - me\", \"bēo - be\", \"gā - go\", \"ðā - then\", \"swīþe - very, \n",
    "# \"thou - you\", \"art - are\"\n",
    "old_eng_stop = [\"þæt\", \"þes\", \"gif\", \"eall\", \"nān\", \"þīn\", \"hēo\", \"swā\", \"mid\", \"fram\", \"þurh\", \"þæt\", \"hwæðer\", \"līc\", \"witan\", \\\n",
    "                \"secgan\", \"cunnan\", \"don\", \"bēon\", \"witan\", \"hæbban\", \"þearf\", \"þeah\", \"sīe\", \"wæs\", \"eom\", \"hæfde\", \"þæt\", \"būtan\", \\\n",
    "                \"eallunga\", \"nǣfre\", \"hwæt\", \"hwī\", \"þæt\", \"læt\", \"wite\", \"wæron\", \"swa\", \"æfre\", \"wyrcan\", \"þinc\", \"gesīen\", \"cūðe\", \\\n",
    "                \"hæle\", \"beon\", \"sēo\", \"dǣl\", \"feorr\", \"bēo\", \"swīþe\", \"thou\", \"art\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d92ea3af-e8d2-4301-9570-a1b4d3194980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using latin stop words from https://www.perseus.tufts.edu/hopper/stopwords which are three or more characters\n",
    "latin_stop = [\"adhic\", \"aliqui\", \"aliquis\", \"ante\", \"apud\", \"atque\", \"aut\", \"autem\", \"cum\", \"cur\", \"deinde\", \"dum\", \"ego\", \"enim\", \"ergo\", \\\n",
    "              \"est\", \"etiam\", \"etsi\", \"fio\", \"haud\", \"hic\", \"iam\", \"idem\", \"igitur\", \"ille\", \"infra\", \"inter\", \"interim\", \"ipse\", \"ita\", \\\n",
    "              \"magis\", \"modo\", \"mox\", \"nam\", \"nec\", \"necque\", \"neque\", \"nisi\", \"non\", \"nos\", \"per\", \"possum\", \"post\", \"pro\", \"quae\", \\\n",
    "              \"quam\", \"quare\", \"qui\", \"quia\", \"quicumque\", \"quidem\", \"quilibet\", \"quis\", \"quisnam\", \"quisquam\", \"quisque\", \"quisquis\", \\\n",
    "              \"quo\", \"quoniam\", \"sed\", \"sic\", \"sive\", \"sub\", \"sui\", \"sum\", \"super\", \"suus\", \"tam\", \"tamen\", \"trans\", \"tum\", \"ubi\", \"uel\", \\\n",
    "              \"uero\", \"unus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c0879d7-9f08-4635-9b2d-24b4fb71137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stop_punct and stopwords\n",
    "stop_final = modern_eng_stop + old_eng_stop + latin_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ba0b482-b43c-476e-92f0-8cbde9870474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopwords(input_tokens):\n",
    "    \"\"\"Removes stop words found in sentence received\"\"\"\n",
    "    return [token for token in input_tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5283562-2320-43fb-b72c-5f231b2e052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words in all of the sentences in cleaned_sentences\n",
    "sentences_nostop = [drop_stopwords(sentence) for sentence in cleaned_sentences]\n",
    "\n",
    "# Removing stop words from cleaned_words list\n",
    "words_nostop = [item for item in cleaned_words if item not in stop_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c43bace4-c602-46fa-bc7d-abf41eed4860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tragedie',\n",
       " 'hamlet',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'actus',\n",
       " 'primus',\n",
       " 'scoena',\n",
       " 'prima',\n",
       " 'enter',\n",
       " 'barnardo']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first ten words in words_nostop\n",
    "words_nostop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dafe968-e366-479e-8413-a7b6530e38ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tragedie', 'hamlet', 'william', 'shakespeare', 'actus', 'primus'],\n",
       " ['scoena', 'prima'],\n",
       " ['enter', 'barnardo', 'francisco', 'centinels'],\n",
       " ['barnardo'],\n",
       " []]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 5 sentences in words_sentences_nostop\n",
    "sentences_nostop[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723774ba-d886-4fe3-b4a4-fcc523b613a9",
   "metadata": {},
   "source": [
    "## Using PorterStemmer or WordNetLemmatizer from nltk.stem on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b8dc018-58b9-4528-a5fe-ba2bee49034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ypolonia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading wordnet in order to use the lemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe3501b0-3514-4758-be47-15773d32fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def perform_lemmatizer(input_token):\n",
    "    return [lemmatizer.lemmatize(token) for token in input_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d64a5c9a-ec42-4f23-b958-f6b058edacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing process for words_sentences_nostop\n",
    "lemmatized_sentences = [perform_lemmatizer(sentence) for sentence in sentences_nostop]\n",
    "\n",
    "# Lemmatizing process for words_nostop\n",
    "lemmatized_words = [lemmatizer.lemmatize(item) for item in words_nostop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df8f9f96-edaf-42e3-89f4-6fcdf9fc182d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tragedie', 'hamlet', 'william', 'shakespeare', 'actus', 'primus']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first lemmatized sentence\n",
    "lemmatized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dde81fe-40d3-4f05-8b96-790ff894aabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tragedie',\n",
       " 'hamlet',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'actus',\n",
       " 'primus',\n",
       " 'scoena',\n",
       " 'prima',\n",
       " 'enter',\n",
       " 'barnardo',\n",
       " 'francisco',\n",
       " 'centinels',\n",
       " 'barnardo',\n",
       " 'fran',\n",
       " 'answer',\n",
       " 'stand',\n",
       " 'vnfold',\n",
       " 'selfe',\n",
       " 'bar',\n",
       " 'liue']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 20 lemmatized words\n",
    "lemmatized_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5f4e1-03a0-466d-b6d2-af9e2a8f4db2",
   "metadata": {},
   "source": [
    "## <span style='color:blue'>Using lemmatization instead of Stemming because unlike stemming, lemmatization considers the context of the word and pursues the goal of returning a meaningful base form of each word.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27487ead-389e-45e1-9ffb-664b24b14f47",
   "metadata": {},
   "source": [
    "## Printing out the words in the first five sentences of the processed text data, to get some ideas of previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f56d0d5b-03ba-48ae-a88f-867858b1a3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tragedie', 'hamlet', 'william', 'shakespeare', 'actus', 'primus'],\n",
       " ['scoena', 'prima'],\n",
       " ['enter', 'barnardo', 'francisco', 'centinels'],\n",
       " ['barnardo'],\n",
       " []]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bec62-9b43-45bf-a42a-c5cfce5268e4",
   "metadata": {},
   "source": [
    "# 2. Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af28901-39bb-4df6-bdf5-29367eaf6b5d",
   "metadata": {},
   "source": [
    "## Creating a CBOW word2vec model from gensim.model. Making choices of vector_size, epochs, window, min_count, and possibly other hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04d502f1-71ea-46fa-b74c-fa6018bb70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CBOW Word2Vec model, using sg = 0 considering CBOW is required\n",
    "cbow_model = Word2Vec(lemmatized_sentences, vector_size = 100, window = 5, min_count = 2, sg = 0, epochs = 10)\n",
    "\n",
    "cbow_model.save('cbow_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab485820-d977-4545-b427-be484134a9a7",
   "metadata": {},
   "source": [
    "## Training the model on the cleaned Shakespeare text data. Use gensim.model.wv.key_to_index and gensim.model.wv.get_vecattr to print out a list of the 20 most frequent words in the vocabulary along with the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d7930d1-6902-4403-b184-a9339061540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Words and Their Counts:\n",
      "\n",
      "Word: 'haue', Count: 448\n",
      "Word: 'ham', Count: 337\n",
      "Word: 'lord', Count: 306\n",
      "Word: 'king', Count: 248\n",
      "Word: 'caesar', Count: 231\n",
      "Word: 'enter', Count: 230\n",
      "Word: 'thy', Count: 202\n",
      "Word: 'thee', Count: 174\n",
      "Word: 'brutus', Count: 162\n",
      "Word: 'vpon', Count: 162\n",
      "Word: 'bru', Count: 153\n",
      "Word: 'hath', Count: 144\n",
      "Word: 'selfe', Count: 138\n",
      "Word: 'macb', Count: 137\n",
      "Word: 'heere', Count: 135\n",
      "Word: 'time', Count: 132\n",
      "Word: 'sir', Count: 121\n",
      "Word: 'night', Count: 119\n",
      "Word: 'speake', Count: 119\n",
      "Word: 'giue', Count: 118\n"
     ]
    }
   ],
   "source": [
    "# Getting the 20 most frequent words\n",
    "most_frequent_words = list(cbow_model.wv.key_to_index.items())[:20]\n",
    "\n",
    "# Printing the words along with their counts\n",
    "print(\"Most Frequent Words and Their Counts:\\n\")\n",
    "for word, index in most_frequent_words:\n",
    "    count = cbow_model.wv.get_vecattr(word, \"count\")\n",
    "    print(f\"Word: '{word}', Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d208a9e-30f4-414f-9899-611a8fe7436a",
   "metadata": {},
   "source": [
    "## Creating a skipgram word2vec model from gensim.model, making choices of vector_size, epochs, window, min_count, and possibly other hyperparameters and training the model on the cleaned Shakespeare text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b59cbee-9df6-4e83-af10-292644fbd9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a skipgram Word2Vec model, using sg = 1 considering skipgram is required\n",
    "skipgram_model = Word2Vec(lemmatized_sentences, vector_size = 100, window = 5, min_count = 2, sg = 1, epochs = 10)\n",
    "\n",
    "skipgram_model.save('skipgram_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8253b0-e01a-4e13-af65-b47c5060f59f",
   "metadata": {},
   "source": [
    "## Loading the pretrained GloVe model from gensim.models.keyedvectors for comparison with the models trained on Shakespeare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11a4e375-3bc4-4cb9-aa32-81472ef10fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading GloVe file\n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "\n",
    "# Loading the GloVe model\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_file, binary = False, no_header = True)\n",
    "\n",
    "glove_model.save('glove_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40d93b-3145-49ac-ae35-194980749794",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>The glove.6B.100d.txt file contains pre-trained word embeddings from the GloVe (Global Vectors for Word Representation) model. Considering the vector size parameter choose for previous models, we are selecting this particular glove file, so there is a correspondence between the input data of each model (data is captured in a 100-dimensional space). In this glove file, each line in the file corresponds to a single word and its vector representation (the file contains vectors for 400,000 words and phrases).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90605aaf-f8ad-47b6-92e4-f98c6b0c99a0",
   "metadata": {},
   "source": [
    "# 3. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78491ce6-ae4d-4212-acbe-b139bf40e0ff",
   "metadata": {},
   "source": [
    "## Comparing the three models by finding the 5 most similar terms to each of the following terms: 'hamlet', 'cauldron', 'nature', 'spirit', 'general', and 'prythee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46d3514f-8d94-4e79-bdf5-9b426a53c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words in CBOW model:\n",
      "\n",
      "Most similar to 'hamlet':\n",
      "hand: 0.9996\n",
      "selfe: 0.9995\n",
      "heere: 0.9995\n",
      "king: 0.9995\n",
      "nature: 0.9995\n",
      "\n",
      "Most similar to 'cauldron':\n",
      "doth: 0.9989\n",
      "euen: 0.9989\n",
      "thee: 0.9989\n",
      "loue: 0.9988\n",
      "giue: 0.9988\n",
      "\n",
      "Most similar to 'nature':\n",
      "hath: 0.9999\n",
      "haue: 0.9999\n",
      "time: 0.9999\n",
      "selfe: 0.9999\n",
      "euen: 0.9998\n",
      "\n",
      "Most similar to 'spirit':\n",
      "haue: 0.9998\n",
      "death: 0.9998\n",
      "hand: 0.9998\n",
      "hath: 0.9998\n",
      "selfe: 0.9998\n",
      "\n",
      "Most similar to 'general':\n",
      "roman: 0.9885\n",
      "braine: 0.9884\n",
      "knowes: 0.9882\n",
      "power: 0.9882\n",
      "england: 0.9881\n",
      "\n",
      "Most similar to 'prythee':\n",
      "spirit: 0.9980\n",
      "loue: 0.9980\n",
      "speech: 0.9980\n",
      "sonne: 0.9980\n",
      "day: 0.9980\n",
      "\n",
      "Most similar words in Skip-gram model:\n",
      "\n",
      "Most similar to 'hamlet':\n",
      "laertes: 0.9928\n",
      "queene: 0.9920\n",
      "polonius: 0.9914\n",
      "macb: 0.9905\n",
      "ham: 0.9905\n",
      "\n",
      "Most similar to 'cauldron':\n",
      "distemper: 0.9986\n",
      "milke: 0.9984\n",
      "ouer: 0.9983\n",
      "scale: 0.9982\n",
      "giuing: 0.9982\n",
      "\n",
      "Most similar to 'nature':\n",
      "magicke: 0.9952\n",
      "heauie: 0.9952\n",
      "mischance: 0.9951\n",
      "distemper: 0.9951\n",
      "vertue: 0.9950\n",
      "\n",
      "Most similar to 'spirit':\n",
      "reuenge: 0.9956\n",
      "mighty: 0.9956\n",
      "wound: 0.9952\n",
      "saide: 0.9952\n",
      "iust: 0.9950\n",
      "\n",
      "Most similar to 'general':\n",
      "braine: 0.9977\n",
      "edge: 0.9976\n",
      "brow: 0.9976\n",
      "seat: 0.9975\n",
      "audience: 0.9975\n",
      "\n",
      "Most similar to 'prythee':\n",
      "brow: 0.9985\n",
      "wayes: 0.9983\n",
      "tearmes: 0.9983\n",
      "ca: 0.9983\n",
      "wide: 0.9983\n",
      "\n",
      "Most similar words in GloVe model:\n",
      "\n",
      "Most similar to 'hamlet':\n",
      "village: 0.6999\n",
      "town: 0.6559\n",
      "situated: 0.5926\n",
      "located: 0.5661\n",
      "unincorporated: 0.5599\n",
      "\n",
      "Most similar to 'cauldron':\n",
      "caldron: 0.7603\n",
      "flame: 0.6907\n",
      "lit: 0.5912\n",
      "torch: 0.5582\n",
      "candle: 0.5477\n",
      "\n",
      "Most similar to 'nature':\n",
      "natural: 0.7198\n",
      "true: 0.7150\n",
      "aspects: 0.7124\n",
      "life: 0.7035\n",
      "view: 0.6961\n",
      "\n",
      "Most similar to 'spirit':\n",
      "passion: 0.7443\n",
      "faith: 0.7213\n",
      "love: 0.6864\n",
      "sense: 0.6724\n",
      "devotion: 0.6692\n",
      "\n",
      "Most similar to 'general':\n",
      "secretary: 0.7607\n",
      "chief: 0.7243\n",
      "gen.: 0.6899\n",
      "president: 0.6798\n",
      "vice: 0.6727\n",
      "'prythee' not found in GloVe vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# List of words to find similar terms for\n",
    "words_to_compare = ['hamlet', 'cauldron', 'nature', 'spirit', 'general', 'prythee']\n",
    "model_names = [\"CBOW\", \"Skip-gram\", \"GloVe\"]\n",
    "\n",
    "def compare_models(models, model_names):\n",
    "    \"\"\"Finds and prints the most similar words\"\"\"\n",
    "    for model, name in zip(models, model_names):\n",
    "        print(f\"\\nMost similar words in {name} model:\")\n",
    "        for word in words_to_compare:\n",
    "            try:\n",
    "                if hasattr(model, 'wv'):    # For Word2Vec models\n",
    "                    similar_terms = model.wv.most_similar(word, topn=5)\n",
    "                else:                       # For GloVe model\n",
    "                    similar_terms = model.most_similar(word, topn=5)\n",
    "\n",
    "                print(f\"\\nMost similar to '{word}':\")\n",
    "                for similar_word, similarity_score in similar_terms:\n",
    "                    print(f\"{similar_word}: {similarity_score:.4f}\")\n",
    "            except KeyError:\n",
    "                print(f\"'{word}' not found in {name} vocabulary.\")\n",
    "\n",
    "compare_models([cbow_model, skipgram_model, glove_model], model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcc3ce-8ed2-47a0-aa4e-1c6afd57c929",
   "metadata": {},
   "source": [
    "## Commenting on how well each model captured the meaning of the word, and if there are multiple meanings, which meaning was given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a532f65-089e-4076-a6c1-289547943b17",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>**'hamlet':** cbow_model and skipgram_model did a great job  linking 'hamlet' to characters from Shakespeare's play, keeping the literary essence alive. In contrast, glove_model seems to think 'hamlet' is more about places, missing the character's importance.</span>\n",
    "\n",
    "### <span style='color:blue'>**'cauldron':** cbow_model gave a mixed bag of unrelated words, while skipgram_model throwed out terms that make you think about what's happening with a cauldron, like 'distemper.' glove_model sticked to physical items related to cauldrons, which makes more sense.</span>\n",
    "\n",
    "### <span style='color:blue'>**'nature':** cbow_model dived into deeper, philosophical ideas, linking 'nature' with different states of being. skip_gram model leaned into the mystical side, while glove_model kept it grounded, connecting 'nature' to real-world concepts.</span>\n",
    "\n",
    "### <span style='color:blue'>**'spirit':** cbow_model tied 'spirit' to deep feelings and existential themes. skipgram_model throwed in words like 'revenge' and 'wound.' glove_model, on the other hand, focused on emotional aspects like 'passion' and 'faith.'</span>\n",
    "\n",
    "### <span style='color:blue'>**'general':** cbow_model connects 'general' with military and political themes. skipgram_model backed this up with hierarchy-related terms. glove_model, however, went broader, which can feel out of place for Shakespeare's time.</span>\n",
    "\n",
    "### <span style='color:blue'>**'prythee':** cbow_model got 'prythee' right by linking it to terms of asking or pleading. skipgram_model did okay but missed the essence of the request. glove_model didn't even recognize 'prythee,' showing it struggled with older words.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe2b21-de9d-4e6e-a96f-a38e1d4e27eb",
   "metadata": {},
   "source": [
    "## Comparing the three models by finding the cosine similarity between the following pairs of terms: ('brutus', 'murder'), ('lady macbeth', 'queen gertrude'), ('fortinbras', 'norway'), ('rome', 'norway'), ('ghost', 'spirit'), ('macbeth', 'hamlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fdce892e-9cd4-4f7a-9d13-5be23bca2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(model, word_1, word_2):\n",
    "    \"\"\"Calculate cosine similarity between two words, based on a model provided.\"\"\"\n",
    "    try:\n",
    "        # Accessing word vectors using the appropriate method\n",
    "        if hasattr(model, 'wv'): # For Word2Vec models\n",
    "            vec1 = model.wv[word_1]\n",
    "            vec2 = model.wv[word_2] \n",
    "        else: # For GloVe model\n",
    "            vec1 = model.get_vector(word_1)\n",
    "            vec2 = model.get_vector(word_2)\n",
    "        \n",
    "        # Calculating cosine similarity\n",
    "        similarity = 1 - cosine(vec1, vec2)\n",
    "        return similarity\n",
    "    except KeyError:\n",
    "        # In case the words are not found in the model\n",
    "        print(f\"One of the words '{word_1}' or '{word_2}' is not in the vocabulary.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e614d429-6af5-4622-8c47-73aa33b48635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the word pairs to compare\n",
    "word_pairs = [\n",
    "    ('brutus', 'murder'),\n",
    "    ('lady macbeth', 'queen gertrude'),\n",
    "    ('fortinbras', 'norway'),\n",
    "    ('rome', 'norway'),\n",
    "    ('ghost', 'spirit'),\n",
    "    ('macbeth', 'hamlet')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b86588a8-8914-46fb-9000-23e633f1e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarities for CBOW model:\n",
      "\n",
      "Similarity between 'brutus' and 'murder': 0.9792\n",
      "One of the words 'lady macbeth' or 'queen gertrude' is not in the vocabulary.\n",
      "One or both words not found in CBOW vocabulary.\n",
      "Similarity between 'fortinbras' and 'norway': 0.9982\n",
      "Similarity between 'rome' and 'norway': 0.9981\n",
      "Similarity between 'ghost' and 'spirit': 0.9989\n",
      "Similarity between 'macbeth' and 'hamlet': 0.9989\n",
      "\n",
      "Cosine Similarities for Skip-gram model:\n",
      "\n",
      "Similarity between 'brutus' and 'murder': 0.9719\n",
      "One of the words 'lady macbeth' or 'queen gertrude' is not in the vocabulary.\n",
      "One or both words not found in Skip-gram vocabulary.\n",
      "Similarity between 'fortinbras' and 'norway': 0.9978\n",
      "Similarity between 'rome' and 'norway': 0.9929\n",
      "Similarity between 'ghost' and 'spirit': 0.9909\n",
      "Similarity between 'macbeth' and 'hamlet': 0.9238\n",
      "\n",
      "Cosine Similarities for GloVe model:\n",
      "\n",
      "Similarity between 'brutus' and 'murder': 0.0736\n",
      "One of the words 'lady macbeth' or 'queen gertrude' is not in the vocabulary.\n",
      "One or both words not found in GloVe vocabulary.\n",
      "Similarity between 'fortinbras' and 'norway': -0.0290\n",
      "Similarity between 'rome' and 'norway': 0.2858\n",
      "Similarity between 'ghost' and 'spirit': 0.4282\n",
      "Similarity between 'macbeth' and 'hamlet': 0.4294\n"
     ]
    }
   ],
   "source": [
    "for model, name in zip([cbow_model, skipgram_model, glove_model], model_names):\n",
    "    # Comparing the three models to find similarities between word pairs given \n",
    "    print(f\"\\nCosine Similarities for {name} model:\\n\")\n",
    "    \n",
    "    for word_1, word_2 in word_pairs:\n",
    "        similarity = cosine_similarity(model, word_1, word_2)\n",
    "        if similarity is not None:\n",
    "            print(f\"Similarity between '{word_1}' and '{word_2}': {similarity:.4f}\")\n",
    "        else:\n",
    "            print(f\"One or both words not found in {name} vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab482541-8693-4a92-a488-8c097a6841fc",
   "metadata": {},
   "source": [
    "## Commenting on how well each model captured the similarity between these terms, especially considering the data that each was trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477fd6a-edab-48c8-8f15-4fd9bc6c9315",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>cbow_model and skipgram_model excelled in capturing the relationships between words in the context of Shakespeare’s plays, likely because they learned directly from the text itself. glove_model, while powerful for general semantic relationships, didn’t quite hit the mark for these specific literary terms, showing the importance of training context in word embeddings.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3670b095-aaeb-4def-b53a-6a6b4cf0d1d1",
   "metadata": {},
   "source": [
    "## Comparing the three models by finding the 5 most similar terms to each of the following word vectors obtained via linear combination: 'denmark' + 'queen', 'scotland' + 'army' + 'general', 'father' - 'man' + 'woman', 'mother' - 'woman' + 'man'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a96809c-d313-4cac-9e4a-c5dd63327e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words in CBOW model:\n",
      "\n",
      "An error occurred for combo ('denmark', 'queen'): invalid index to scalar variable.\n",
      "\n",
      "Similar terms for ('scotland', 'army', 'general'):\n",
      "clau: 0.3076\n",
      "slept: 0.1897\n",
      "sparkes: 0.1887\n",
      "couch: 0.1857\n",
      "drawne: 0.1819\n",
      "\n",
      "Similar terms for ('father', 'man', 'woman'):\n",
      "father: 0.9999\n",
      "thy: 0.9998\n",
      "haue: 0.9997\n",
      "vpon: 0.9997\n",
      "death: 0.9997\n",
      "\n",
      "Similar terms for ('mother', 'woman', 'man'):\n",
      "mother: 0.9991\n",
      "thee: 0.9985\n",
      "god: 0.9985\n",
      "night: 0.9985\n",
      "brother: 0.9985\n",
      "\n",
      "Most similar words in Skip-gram model:\n",
      "\n",
      "An error occurred for combo ('denmark', 'queen'): invalid index to scalar variable.\n",
      "\n",
      "Similar terms for ('scotland', 'army', 'general'):\n",
      "clau: 0.3119\n",
      "pind: 0.1618\n",
      "finis: 0.1440\n",
      "puh: 0.1414\n",
      "preferre: 0.1345\n",
      "\n",
      "Similar terms for ('father', 'man', 'woman'):\n",
      "father: 0.9959\n",
      "mother: 0.9883\n",
      "lost: 0.9879\n",
      "wife: 0.9855\n",
      "woman: 0.9832\n",
      "\n",
      "Similar terms for ('mother', 'woman', 'man'):\n",
      "mother: 0.9960\n",
      "wife: 0.9891\n",
      "ayde: 0.9888\n",
      "weepe: 0.9887\n",
      "father: 0.9886\n",
      "\n",
      "Most similar words in GloVe model:\n",
      "\n",
      "An error occurred for combo ('denmark', 'queen'): invalid index to scalar variable.\n",
      "\n",
      "Similar terms for ('scotland', 'army', 'general'):\n",
      "unit: 0.4130\n",
      "commander: 0.4115\n",
      "entire: 0.3916\n",
      "trunk: 0.3827\n",
      "kato: 0.3825\n",
      "\n",
      "Similar terms for ('father', 'man', 'woman'):\n",
      "mother: 0.9137\n",
      "daughter: 0.8749\n",
      "father: 0.8659\n",
      "wife: 0.8636\n",
      "husband: 0.8385\n",
      "\n",
      "Similar terms for ('mother', 'woman', 'man'):\n",
      "father: 0.8900\n",
      "brother: 0.8525\n",
      "mother: 0.8325\n",
      "son: 0.8202\n",
      "man: 0.8150\n"
     ]
    }
   ],
   "source": [
    "# Defining the word pairs for linear combinations\n",
    "word_combinations = [\n",
    "    ('denmark', 'queen'),\n",
    "    ('scotland', 'army', 'general'),\n",
    "    ('father', 'man', 'woman'),\n",
    "    ('mother', 'woman', 'man')\n",
    "]\n",
    "\n",
    "def find_similar_terms(models, model_names, word_combinations):\n",
    "    \"\"\"Finds most similar words for each model based on linear combinations\"\"\"\n",
    "    \n",
    "    for model, name in zip(models, model_names):\n",
    "        print(f\"\\nMost similar words in {name} model:\\n\")\n",
    "\n",
    "        for index, combo in enumerate(word_combinations):\n",
    "            try:\n",
    "                if hasattr(model, 'wv'):  # For Word2Vec models\n",
    "                    vec1 = model.wv[combo[0]]\n",
    "                    vec2 = model.wv[combo[1]]\n",
    "                    if len(combo) == 3:\n",
    "                        vec3 = model.wv[combo[2]]\n",
    "                    else:\n",
    "                        vec3 = 0  # No third vector, set to 0\n",
    "\n",
    "                else:  # For GloVe model\n",
    "                    vec1 = model.get_vector(combo[0])\n",
    "                    vec2 = model.get_vector(combo[1])\n",
    "                    if len(combo) == 3:\n",
    "                        vec3 = model.get_vector(combo[2])\n",
    "                    else:\n",
    "                        vec3 = 0  # No third vector, set to 0\n",
    "\n",
    "               # Computing the resulting vector\n",
    "                if index < 2:\n",
    "                    vector = sum((vec1 + vec2), vec3)\n",
    "                else:\n",
    "                    vector = vec1 - vec2 + vec3\n",
    "\n",
    "                # Finding the most similar words\n",
    "                if hasattr(model, 'wv'):  # For Word2Vec models\n",
    "                    similar_terms = model.wv.similar_by_vector(vector, topn=5)\n",
    "                else:  # For GloVe model\n",
    "                    similar_terms = model.similar_by_vector(vector, topn=5)\n",
    "\n",
    "                print(f\"\\nSimilar terms for {combo}:\")\n",
    "                for term, similarity in similar_terms:\n",
    "                    print(f\"{term}: {similarity:.4f}\")\n",
    "\n",
    "            except KeyError as e:\n",
    "                print(f\"One of the words {combo} not found in {name} vocabulary: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred for combo {combo}: {e}\")\n",
    "\n",
    "find_similar_terms([cbow_model, skipgram_model, glove_model], model_names, word_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c022165-c30a-447d-8460-b11c7dc4c349",
   "metadata": {},
   "source": [
    "## Commenting on how well each model described the ideas behind these word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa23b8e-017a-4a45-9e57-a6ba7a01c0e4",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>While each model has its strengths, they all have room for improvement, especially when it comes to handling tricky word pairings. Their successes show they can capture family relationships, but they need to work on understanding deeper or more nuanced ideas. Evaluating each combo results, we have:</span>\n",
    "\n",
    "### <span style='color:blue'>Insights for cbow_model:</span>\n",
    "* <span style='color:blue'>Error for ('denmark' + 'queen'): The model couldn't process this combo correctly, probably because it couldn't figure out how to combine those words properly.</span>\n",
    "* <span style='color:blue'>('scotland' + 'army' + 'general'): The words it returned feel a bit random, not really capturing the military essence that we were hoping for.</span>\n",
    "* <span style='color:blue'>('father' - 'man' + 'woman'): This one worked pretty well! It really focused on \"father,\" showing it gets the male side of family relationships.</span>\n",
    "* <span style='color:blue'>('mother' - 'woman' + 'man'): Here, it did well too, strongly linking to \"mother,\" which fits with the nurturing role traditionally associated with women.</span>\n",
    "\n",
    "### <span style='color:blue'>Insights for skipgram_model:</span>\n",
    "* <span style='color:blue'>Error for ('denmark' + 'queen'): Same story here; it couldn’t process this combo correctly.</span>\n",
    "* <span style='color:blue'>('scotland' + 'army' + 'general'): The results were still a bit off, suggesting it struggled to grasp the military context like the cbow_model.</span>\n",
    "* <span style='color:blue'>('father' - 'man' + 'woman'): This model fared better with family terms, emphasizing \"father\" and also including \"mother\" and \"wife,\" showing it has a decent grasp on gender roles.</span>\n",
    "* <span style='color:blue'>('mother' - 'woman' + 'man'): Here, it did well too, strongly linking to \"mother,\" which fits with the nurturing role traditionally associated with women.</span>\n",
    "\n",
    "### <span style='color:blue'>Insights for glove_model:</span>\n",
    "* <span style='color:blue'>Error for ('denmark' + 'queen'): The model couldn't process this combo correctly as in previous models.</span>\n",
    "* <span style='color:blue'>('scotland' + 'army' + 'general'): TThis model did a bit better with military terms, offering up words like \"unit\" and \"commander,\" though they still felt a bit generic.</span>\n",
    "* <span style='color:blue'>('father' - 'man' + 'woman'): It gave relevant words like \"mother\" and \"daughter,\" showing it understands family relationships, but it leans towards traditional roles.</span>\n",
    "* <span style='color:blue'>('mother' - 'woman' + 'man'): It did a good job here, associating with \"father\" and \"brother,\" showing it gets family dynamics pretty well.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ecd2e-7031-48d4-8c40-054e29ebec6b",
   "metadata": {},
   "source": [
    "## Overall comments on how each model performs, describing what data you would use to train a better word embedding model to captures the meaning of Shakespearean English\n",
    "\n",
    "### <span style='color:blue'>The cbow_model and skipgram_model showed strong potential in recognizing relationships within the context of Shakespeare’s plays, thanks to their training on relevant texts. However, both models could benefit from broader datasets that include historical and thematic context. The glove_model, while useful for general semantic relationships, fell short in understanding the specific literary nuances, highlighting the importance of context in word embeddings. By expanding their training data to include more comprehensive Shakespearean texts and related literature, these models could significantly improve their performance in capturing the richness of Shakespearean English.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64164e5-d102-4a75-a67d-d99a0f1bb4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
