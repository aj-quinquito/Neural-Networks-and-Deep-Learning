{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Data\n",
    "\n",
    "a. Use nltk.corpus.gutenberg.raw to load the three plays listed above into a single variable and lower the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine the three plays into a single text\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "macbeth = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "julius_caesar = gutenberg.raw('shakespeare-caesar.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the tragedie of hamlet by william shakespeare 1599]\n",
      "\n",
      "\n",
      "actus primus. scoena prima.\n",
      "\n",
      "enter barnardo and francisco two centinels.\n",
      "\n",
      "  barnardo. who's there?\n",
      "  fran. nay answer me: stand & vnfold\n",
      "your selfe\n",
      "\n",
      "   bar. long liue the king\n",
      "\n",
      "   fran. barnardo?\n",
      "  bar. he\n",
      "\n",
      "   fran. you come most carefully vpon your houre\n",
      "\n",
      "   bar. 'tis now strook twelue, get thee to bed francisco\n",
      "\n",
      "   fran. for this releefe much thankes: 'tis bitter cold,\n",
      "and i am sicke at heart\n",
      "\n",
      "   barn. haue you had quiet guard?\n",
      "  fran. not\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing the text to normalize it\n",
    "combined_text = (hamlet + macbeth + julius_caesar).lower()\n",
    "\n",
    "print(combined_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Perform the following steps in an order of your choosing:\n",
    "\n",
    "- Tokenize the text into sentences, and then each sentence into words.\n",
    "- Use Speller from the autocorrect library to correct spelling mistakes. \n",
    "- Create a list of stopwords (using publicly available lists and/or adding your own) and remove these.\n",
    "- Use PorterStemmer or WordNetLemmatizer from nltk.stem on the text.\n",
    "- Use regular expressions (the re library) to do any additional cleanup of the text you wish to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into sentences and words\n",
    "sentences = sent_tokenize(combined_text)\n",
    "words = [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct spelling mistakes using Speller from the autocorrect library\n",
    "spell = Speller()\n",
    "words_corrected = [[spell(word) for word in sentence] for sentence in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = {'thou', 'thee', 'thy', 'hath'}  \n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "words_no_stopwords = [[word for word in sentence if word not in stop_words] for sentence in words_corrected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the words using WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_lemmatized = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in words_no_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to clean up non-alphabetic tokens\n",
    "words_cleaned = [[re.sub(r'\\W+', '', word) for word in sentence if word.isalpha()] for sentence in words_lemmatized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Print out the words in the first five sentences of the processed text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tragedy', 'hamlet', 'william', 'shakespeare', 'act', 'prime'], ['scene', 'prima'], ['enter', 'bernard', 'francisco', 'two', 'sentinel'], ['bernard'], []]\n"
     ]
    }
   ],
   "source": [
    "# Print the first five processed sentences for review\n",
    "print(words_cleaned[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Modeling\n",
    "\n",
    "a. Create a CBOW word2vec model from gensim.model. Make choices of vector_size, epochs, window, min_count, and possibly other hyperparameters. Train it on the cleaned Shakespeare text data. Use gensim.model.wv.key_to_index  and gensim.model.wv.get_vecattr to print out a list of the 20 most frequent words in the vocabulary along with the word count. Consider improving the text cleaning steps above based on this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for the CBOW model\n",
    "vector_size = 100  # Dimensionality of word vectors\n",
    "window = 5  # Context window size\n",
    "min_count = 2  # Ignores words with total frequency lower than this\n",
    "epochs = 10  # Number of iterations over the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CBOW Word2Vec model (sg=0 indicates CBOW model)\n",
    "cbow_model = Word2Vec(sentences=words_cleaned, vector_size=vector_size, window=window, min_count=min_count, sg=0, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent words and their counts:\n",
      "ham: 337\n",
      "lord: 306\n",
      "shall: 300\n",
      "come: 284\n",
      "king: 248\n",
      "enter: 230\n",
      "good: 221\n",
      "let: 220\n",
      "mac: 205\n",
      "like: 200\n",
      "cesar: 193\n",
      "one: 188\n",
      "make: 185\n",
      "know: 184\n",
      "v: 175\n",
      "self: 165\n",
      "would: 162\n",
      "aboutus: 162\n",
      "von: 160\n",
      "go: 159\n"
     ]
    }
   ],
   "source": [
    "# Get the 20 most frequent words from the vocabulary along with their counts\n",
    "most_frequent_words = list(cbow_model.wv.key_to_index.items())[:20]\n",
    "\n",
    "# Print the most frequent words and their word counts\n",
    "print(\"Top 20 most frequent words and their counts:\")\n",
    "for word, index in most_frequent_words:\n",
    "    word_count = cbow_model.wv.get_vecattr(word, \"count\")\n",
    "    print(f\"{word}: {word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Create a skipgram word2vec model from gensim.model. Make choices of vector_size, epochs, window, min_count, and possibly other hyperparameters. Train it on the cleaned Shakespeare text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Skipgram Word2Vec model (sg=1 for Skipgram)\n",
    "skipgram_model = Word2Vec(sentences=words_cleaned, vector_size=vector_size, window=window, min_count=min_count, sg=1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1e1bf477ac0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Load the pretrained GloVe model from gensim.models.keyedvectors for comparison with the models trained on Shakespeare text. Use markdown to make note of the data that GloVe has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1e1bdc27340>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GloVe file \n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "\n",
    "# Load the GloVe model\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "\n",
    "# Print a summary of the GloVe model\n",
    "glove_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Discussion\n",
    "\n",
    "a. Compare the three models by finding the 5 most similar terms to each of the following terms: 'hamlet', 'cauldron', 'nature', 'spirit', 'general', and 'prythee'. Comment on how well each model captured the meaning of the word, and if there are multiple meanings, which meaning was given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Word                                                                CBOW                                                                  Skipgram                                                                                             GloVe\n",
      "  hamlet     king: 1.00 | good: 1.00 | till: 1.00 | give: 1.00 | faire: 1.00          queen: 0.99 | king: 0.99 | colony: 0.98 | lord: 0.98 | ham: 0.98                village: 0.70 | town: 0.66 | situated: 0.59 | located: 0.57 | unincorporated: 0.56\n",
      "cauldron   one: 1.00 | thought: 1.00 | stroke: 1.00 | man: 1.00 | poor: 1.00       memory: 1.00 | cool: 1.00 | bubble: 1.00 | slow: 1.00 | share: 1.00                              caldron: 0.76 | flame: 0.69 | lit: 0.59 | torch: 0.56 | candle: 0.55\n",
      "  nature whose: 1.00 | thought: 1.00 | thing: 1.00 | may: 1.00 | world: 1.00          yes: 0.99 | seems: 0.99 | hold: 0.99 | state: 0.99 | whose: 0.99                              natural: 0.72 | true: 0.71 | aspects: 0.71 | life: 0.70 | view: 0.70\n",
      "  spirit      take: 1.00 | may: 1.00 | shall: 1.00 | made: 1.00 | self: 1.00 ordinary: 0.99 | leader: 0.99 | labor: 0.99 | start: 0.99 | respect: 0.99                           passion: 0.74 | faith: 0.72 | love: 0.69 | sense: 0.67 | devotion: 0.67\n",
      " general      best: 1.00 | soul: 1.00 | hand: 1.00 | might: 1.00 | hee: 1.00      legion: 1.00 | affair: 1.00 | speaker: 1.00 | apply: 1.00 | ne: 1.00                         secretary: 0.76 | chief: 0.72 | gen.: 0.69 | president: 0.68 | vice: 0.67\n",
      " prythee  may: 1.00 | well: 1.00 | league: 1.00 | made: 1.00 | nothing: 1.00        suite: 1.00 | breech: 1.00 | saying: 1.00 | fluid: 1.00 | ne: 1.00 Not in vocabulary | Not in vocabulary | Not in vocabulary | Not in vocabulary | Not in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# List of target words for comparison\n",
    "target_words = ['hamlet', 'cauldron', 'nature', 'spirit', 'general', 'prythee']\n",
    "\n",
    "# Function to get most similar words from a model\n",
    "def get_most_similar(model, word):\n",
    "    try:\n",
    "        return [f\"{sim_word}: {sim_score:.2f}\" for sim_word, sim_score in model.most_similar(word, topn=5)]\n",
    "    except KeyError:\n",
    "        return [\"Not in vocabulary\"] * 5  # Handle missing vocabulary gracefully\n",
    "\n",
    "# Initialize a dictionary to hold the results\n",
    "results = {\"Word\": target_words}\n",
    "\n",
    "# Get results for each model and store them in the dictionary\n",
    "cbow_similarities = [get_most_similar(cbow_model.wv, word) for word in target_words]\n",
    "skipgram_similarities = [get_most_similar(skipgram_model.wv, word) for word in target_words]\n",
    "glove_similarities = [get_most_similar(glove_model, word) for word in target_words]\n",
    "\n",
    "# Convert the lists into columns for the table\n",
    "results['CBOW'] = [' | '.join(similar) for similar in cbow_similarities]\n",
    "results['Skipgram'] = [' | '.join(similar) for similar in skipgram_similarities]\n",
    "results['GloVe'] = [' | '.join(similar) for similar in glove_similarities]\n",
    "\n",
    "# Convert the results dictionary into a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Display the table\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Compare the three models by finding the cosine similarity between the following pairs of terms: ('brutus', 'murder'), ('lady macbeth', 'queen gertrude'), ('fortinbras', 'norway'), ('rome', 'norway'), ('ghost', 'spirit'), ('macbeth', 'hamlet'). Comment on how well each model captured the similarity between these terms, especially considering the data that each was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Word Pair      CBOW  Skipgram     GloVe\n",
      "              brutus - murder       N/A       N/A  0.073644\n",
      "lady macbeth - queen gertrude       N/A       N/A       N/A\n",
      "          fortinbras - norway  0.999197  0.996554 -0.028962\n",
      "                rome - norway  0.999187  0.983509  0.285837\n",
      "               ghost - spirit  0.998928  0.984075  0.428209\n",
      "             macbeth - hamlet  0.998807  0.887623  0.429359\n"
     ]
    }
   ],
   "source": [
    "# List of word pairs to compare\n",
    "word_pairs = [\n",
    "    ('brutus', 'murder'),\n",
    "    ('lady macbeth', 'queen gertrude'),\n",
    "    ('fortinbras', 'norway'),\n",
    "    ('rome', 'norway'),\n",
    "    ('ghost', 'spirit'),\n",
    "    ('macbeth', 'hamlet')\n",
    "]\n",
    "\n",
    "# Function to compute cosine similarity between two words for a given model\n",
    "def get_cosine_similarity(model, word1, word2):\n",
    "    try:\n",
    "        return model.similarity(word1, word2)\n",
    "    except KeyError:\n",
    "        return \"N/A\"  # Return \"N/A\" if one or both words are not in the model's vocabulary\n",
    "\n",
    "# Dictionary to hold results for the DataFrame\n",
    "results = {'Word Pair': [f\"{word1} - {word2}\" for word1, word2 in word_pairs]}\n",
    "\n",
    "# Get cosine similarities for each model\n",
    "results['CBOW'] = [get_cosine_similarity(cbow_model.wv, word1, word2) for word1, word2 in word_pairs]\n",
    "results['Skipgram'] = [get_cosine_similarity(skipgram_model.wv, word1, word2) for word1, word2 in word_pairs]\n",
    "results['GloVe'] = [get_cosine_similarity(glove_model, word1, word2) for word1, word2 in word_pairs]\n",
    "\n",
    "# Convert the results dictionary into a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Display the table\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Compare the three models by finding the 5 most similar terms to each of the following word vectors obtained via linear combination: 'denmark' + 'queen', 'scotland' + 'army' + 'general', 'father' - 'man' + 'woman', 'mother' - 'woman' + 'man'. Comment on how well each model described the ideas behind these word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Combination                                                                CBOW                                                                Skipgram                                                                                               GloVe\n",
      "            'denmark' + 'queen'       queen: 1.0 | denmark: 1.0 | take: 1.0 | king: 1.0 | till: 1.0 ophelia: 1.0 | deer: 1.0 | sweet: 0.99 | alert: 0.99 | donalbaine: 0.99                          queen: 0.86 | denmark: 0.84 | sweden: 0.74 | norway: 0.69 | princess: 0.69\n",
      "'scotland' + 'army' + 'general'          general: 1.0 | put: 1.0 | hee: 1.0 | hand: 1.0 | name: 1.0        ne: 1.0 | pindarus: 1.0 | rule: 1.0 | legion: 1.0 | silence: 1.0                            army: 0.83 | general: 0.81 | force: 0.76 | military: 0.75 | forces: 0.74\n",
      "     'father' - 'man' + 'woman' cl: 0.1 | descends: 0.07 | donald: -0.02 | hec: -0.09 | stra: -0.09    macbeth: 0.18 | father: 0.17 | enter: 0.15 | cl: 0.14 | hamlet: 0.13 daughter-in-law: 0.56 | remarried: 0.56 | sister-in-law: 0.56 | step-father: 0.55 | satyavati: 0.54\n",
      "     'mother' - 'woman' + 'man'           mother: 1.0 | man: 1.0 | th: 1.0 | god: 1.0 | friend: 1.0             wee: 0.02 | l: 0.02 | come: 0.0 | father: -0.0 | stra: -0.0                          darius: 0.47 | nat: 0.45 | theodoric: 0.43 | o'donovan: 0.42 | neill: 0.42\n"
     ]
    }
   ],
   "source": [
    "# List of linear word combinations for comparison\n",
    "vector_combinations = [\n",
    "    (['denmark', 'queen'], []),  # 'denmark' + 'queen'\n",
    "    (['scotland', 'army', 'general'], []),  # 'scotland' + 'army' + 'general'\n",
    "    (['father', 'woman'], ['man']),  # 'father' - 'man' + 'woman'\n",
    "    (['mother', 'man'], ['woman'])   # 'mother' - 'woman' + 'man'\n",
    "]\n",
    "\n",
    "# Function to get the most similar terms to a vector combination, with vector averaging for multiple words\n",
    "def get_similar_terms(model, positives, negatives):\n",
    "    try:\n",
    "        # If multiple positive words, average their vectors\n",
    "        if len(positives) > 1:\n",
    "            vector = sum([model[word] for word in positives]) / len(positives)\n",
    "            # Adding negative vector handling\n",
    "            if negatives:\n",
    "                vector -= sum([model[word] for word in negatives]) / len(negatives)\n",
    "            return model.similar_by_vector(vector, topn=5)\n",
    "        else:\n",
    "            return model.most_similar(positive=positives, negative=negatives, topn=5)\n",
    "    except KeyError:\n",
    "        return \"One or more words not in vocabulary\"\n",
    "\n",
    "# Dictionary to hold results for the DataFrame\n",
    "results = {'Combination': [\"'denmark' + 'queen'\", \"'scotland' + 'army' + 'general'\", \"'father' - 'man' + 'woman'\", \"'mother' - 'woman' + 'man'\"]}\n",
    "\n",
    "# Get most similar terms for each model and each word vector combination\n",
    "results['CBOW'] = [get_similar_terms(cbow_model.wv, positives, negatives) for positives, negatives in vector_combinations]\n",
    "results['Skipgram'] = [get_similar_terms(skipgram_model.wv, positives, negatives) for positives, negatives in vector_combinations]\n",
    "results['GloVe'] = [get_similar_terms(glove_model, positives, negatives) for positives, negatives in vector_combinations]\n",
    "\n",
    "# Format results for table display (convert list of tuples to string)\n",
    "def format_similar_terms(similar_terms):\n",
    "    if isinstance(similar_terms, str):\n",
    "        return similar_terms  # In case of KeyError (not found in vocabulary)\n",
    "    return ' | '.join([f\"{term}: {round(similarity, 2)}\" for term, similarity in similar_terms])\n",
    "\n",
    "# Apply formatting to the DataFrame\n",
    "results['CBOW'] = [format_similar_terms(sim) for sim in results['CBOW']]\n",
    "results['Skipgram'] = [format_similar_terms(sim) for sim in results['Skipgram']]\n",
    "results['GloVe'] = [format_similar_terms(sim) for sim in results['GloVe']]\n",
    "\n",
    "# Convert the results dictionary into a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Display the table\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Give overall comments on how each model performs. Describe what data you would use to train a better word embedding model to captures the meaning of Shakespearean English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
